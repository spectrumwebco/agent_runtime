apiVersion: v1
kind: ServiceAccount
metadata:
  name: kserve
  namespace: kserve
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: kserve-config
  namespace: kserve
data:
  inferenceservice: |
    {
      "storageInitializer": {
        "image": "kserve/storage-initializer:latest",
        "memoryRequest": "100Mi",
        "memoryLimit": "1Gi",
        "cpuRequest": "100m",
        "cpuLimit": "1"
      }
    }
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama4-inference
  namespace: kserve
  annotations:
    vault.hashicorp.com/agent-inject: "true"
    vault.hashicorp.com/agent-inject-secret-kserve-config: "kv/data/kubeflow/config"
    vault.hashicorp.com/agent-inject-template-kserve-config: |
      {{- with secret "kv/data/kubeflow/config" -}}
      export MLFLOW_TRACKING_URI="{{ .Data.data.mlflow_tracking_uri }}"
      export S3_ENDPOINT_URL="{{ .Data.data.s3_endpoint_url }}"
      {{- end -}}
    vault.hashicorp.com/role: "kserve"
spec:
  predictor:
    serviceAccountName: kserve
    containers:
      - name: kserve-container
        image: fine-tune/llama4-inference:latest
        command:
          - "/bin/sh"
          - "-c"
          - |
            source /vault/secrets/kserve-config
            python -m inference_server \
              --model-path /mnt/models/llama4-maverick \
              --mlflow-tracking-uri $MLFLOW_TRACKING_URI
        resources:
          limits:
            nvidia.com/gpu: 1
            memory: "16Gi"
            cpu: "4"
          requests:
            memory: "8Gi"
            cpu: "2"
        volumeMounts:
          - name: models-volume
            mountPath: /mnt/models
    volumes:
      - name: models-volume
        persistentVolumeClaim:
          claimName: models-pvc
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kserve-ingress
  namespace: kserve
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - host: kserve.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: llama4-inference-predictor
            port:
              number: 8080
